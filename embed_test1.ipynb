{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96e0d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1234)\n",
    "import pickle\n",
    "import argparse\n",
    "from load_data import *\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "MODEL_TYPE = \"bert-base-uncased\"\n",
    "BATCH_SIZE = 16 #32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a345860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pre-train tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)\n",
    "# tokenizer1, input_ids, attention_masks = load_data_BERT(\"nyt\", tokenizer, with_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f0f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output_name = \"nyt.pkl\"\n",
    "outp = open(output_name, \"rb\")\n",
    "outp_dict = pickle.load(outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32bd7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts = sum(\n",
    "    outp_dict[\"word_counts\"][ele] for ele in outp_dict[\"word_counts\"]\n",
    ")\n",
    "total_counts -= outp_dict[\"word_counts\"][outp_dict[\"vocabulary_inv_list\"][0]]\n",
    "background_array = np.zeros(outp_dict[\"vocab_sz\"])\n",
    "for i in range(1, outp_dict[\"vocab_sz\"]):\n",
    "    background_array[i] = (\n",
    "        outp_dict[\"word_counts\"][outp_dict[\"vocabulary_inv\"][i]] / total_counts\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "984f920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceed_level(x, sequences, wstc, args, pretrain_epochs, self_lr, decay, update_interval,\n",
    "                delta, class_tree, level, expand_num, background_array, doc_length, sent_length, len_avg,\n",
    "                len_std, num_doc, interp_weight, vocabulary_inv, common_words):\n",
    "    print(f\"\\n### Proceeding level {level} ###\")\n",
    "    dataset = args.dataset\n",
    "    sup_source = args.sup_source\n",
    "    maxiter = args.maxiter.split(',')\n",
    "    maxiter = int(maxiter[level])\n",
    "    batch_size = args.batch_size\n",
    "    parents = class_tree.find_at_level(level)\n",
    "    parents_names = [parent.name for parent in parents]\n",
    "    print(f'Nodes: {parents_names}')\n",
    "    \n",
    "    for parent in parents:\n",
    "        # initialize classifiers in hierarchy\n",
    "        print(\"\\n### Input preparation ###\")\n",
    "\n",
    "        if class_tree.embedding is None:\n",
    "            train_class_embedding(x, vocabulary_inv, dataset_name=args.dataset, node=class_tree)\n",
    "        parent.embedding = class_tree.embedding\n",
    "        wstc.instantiate(class_tree=parent)\n",
    "        \n",
    "        save_dir = f'./results/{dataset}/{sup_source}/level_{level}'\n",
    "\n",
    "        if parent.model is not None:\n",
    "            \n",
    "            print(\"\\n### Phase 1: vMF distribution fitting & pseudo document generation ###\")\n",
    "\n",
    "            if args.pseudo == \"bow\":\n",
    "                print(\"Pseudo documents generation (Method: Bag-of-words)...\")\n",
    "                seed_docs, seed_label = bow_pseudodocs(parent.children, expand_num, background_array, doc_length, len_avg,\n",
    "                                                        len_std, num_doc, interp_weight, vocabulary_inv, parent.embedding, save_dir)\n",
    "            elif args.pseudo == \"lstm\":\n",
    "                print(\"Pseudo documents generation (Method: LSTM language model)...\")\n",
    "                lm = train_lstm(sequences, common_words, sent_length, f'./{dataset}/lm', embedding_matrix=class_tree.embedding)\n",
    "                \n",
    "                seed_docs, seed_label = lstm_pseudodocs(parent, expand_num, doc_length, len_avg, sent_length, len_std, num_doc, \n",
    "                                                        interp_weight, vocabulary_inv, lm, common_words, save_dir)\n",
    "            \n",
    "            print(\"Finished pseudo documents generation.\")\n",
    "            num_real_doc = len(seed_docs) / 5\n",
    "\n",
    "            if sup_source == 'docs':\n",
    "                real_seed_docs, real_seed_label = augment(x, parent.children, num_real_doc)\n",
    "                print(f'Labeled docs {len(real_seed_docs)} + Pseudo docs {len(seed_docs)}')\n",
    "                seed_docs = np.concatenate((seed_docs, real_seed_docs), axis=0)\n",
    "                seed_label = np.concatenate((seed_label, real_seed_label), axis=0)\n",
    "\n",
    "            perm = np.random.permutation(len(seed_label))\n",
    "            seed_docs = seed_docs[perm]\n",
    "            seed_label = seed_label[perm]\n",
    "            \n",
    "            print(seed_docs, seed_label)\n",
    "\n",
    "            print('\\n### Phase 2: pre-training with pseudo documents ###')\n",
    "            print(f'Pretraining node {parent.name}')\n",
    "\n",
    "#             wstc.pretrain(x=seed_docs, pretrain_labels=seed_label, model=parent.model,\n",
    "#                         optimizer=SGD(lr=0.1, momentum=0.9),\n",
    "#                         epochs=pretrain_epochs, batch_size=batch_size,\n",
    "#                         save_dir=save_dir, suffix=parent.name)\n",
    "\n",
    "#     global_classifier = wstc.ensemble_classifier(level)\n",
    "#     wstc.model.append(global_classifier)\n",
    "#     t0 = time()\n",
    "#     print(\"\\n### Phase 3: self-training ###\")\n",
    "#     selftrain_optimizer = SGD(lr=self_lr, momentum=0.9, decay=decay)\n",
    "#     wstc.compile(level, optimizer=selftrain_optimizer, loss='kld')\n",
    "#     y_pred = wstc.fit(x, level=level, tol=delta, maxiter=maxiter, batch_size=batch_size,\n",
    "#                       update_interval=update_interval, save_dir=save_dir)\n",
    "#     print(f'Self-training time: {time() - t0:.2f}s')\n",
    "#     return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "565aaefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\NER\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\User\\anaconda3\\envs\\NER\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from models_BERT import WSTC\n",
    "wstc = WSTC(\n",
    "    input_shape=outp_dict[\"x\"].shape,\n",
    "    class_tree=outp_dict[\"class_tree\"],\n",
    "    sup_source=outp_dict[\"args\"].sup_source,\n",
    "    y=outp_dict[\"y\"],\n",
    "    vocab_sz=outp_dict[\"vocab_sz\"],\n",
    "    block_thre=outp_dict[\"args\"].gamma,\n",
    "    block_level=outp_dict[\"args\"].block_level,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8c2be27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Proceeding level 0 ###\n",
      "Nodes: ['ROOT']\n",
      "\n",
      "### Input preparation ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Phase 1: vMF distribution fitting & pseudo document generation ###\n",
      "Pseudo documents generation (Method: LSTM language model)...\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 39)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 39, 100)           1000100   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 39, 100)           80400     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10001)             1010101   \n",
      "=================================================================\n",
      "Total params: 2,171,001\n",
      "Trainable params: 1,170,901\n",
      "Non-trainable params: 1,000,100\n",
      "_________________________________________________________________\n",
      "Loading model ./nyt/lm/model-final.h5...\n",
      "Retrieving top-t nearest words...\n",
      "current_sz: 3\n",
      "Final expansion size t = 3\n",
      "['cuts', 'budget', 'debt']\n",
      "['surveillance', 'intelligence', 'snowden']\n",
      "['insurance', 'coverage', 'medicaid']\n",
      "['immigrants', 'immigration', 'citizenship']\n",
      "['death', 'judge', 'prosecutors']\n",
      "['gay', 'marriage', 'same-sex']\n",
      "['gun', 'guns', 'rifle']\n",
      "['military', 'pentagon', 'combat']\n",
      "['abortion', 'abortions', 'clinics']\n",
      "weight: [0.14814815 0.11111111 0.18518519 0.07407407 0.11111111 0.11111111\n",
      " 0.07407407 0.07407407 0.11111111]\n",
      "kappa: [480.33168838 420.13272182 299.88498376 373.7476451  545.15124715\n",
      " 303.34224693 773.90830373 666.21531631 376.6760087 ]\n",
      "['ballet', 'dancers', 'dancer']\n",
      "['episode', 'viewers', 'episodes']\n",
      "['album', 'songs', 'orchestra']\n",
      "['hollywood', 'directed', 'oscar']\n",
      "weight: [0.25       0.08333333 0.33333333 0.33333333]\n",
      "kappa: [3.59911749e+02 1.00000000e+10 3.03910148e+02 1.90647498e+02]\n",
      "['stocks', 'dow', 'points']\n",
      "['natural', 'power', 'electricity']\n",
      "['fed', 'economists', 'economist']\n",
      "['china', 'union', 'euro']\n",
      "weight: [0.25       0.25       0.33333333 0.16666667]\n",
      "kappa: [246.71997946 337.82188023 270.01941823 184.20523846]\n",
      "['spacecraft', 'sun', 'kepler']\n",
      "['climate', 'wildlife', 'fish']\n",
      "weight: [0.5 0.5]\n",
      "kappa: [269.05010271 375.99381647]\n",
      "['period', 'rangers', 'bruins']\n",
      "['rebounds', 'nets', 'knicks']\n",
      "['wimbledon', 'tennis', 'slam']\n",
      "['golf', 'tour', 'pga']\n",
      "['yards', 'quarterback', 'touchdown']\n",
      "['innings', 'inning', 'yankees']\n",
      "['cup', 'champions', 'united']\n",
      "weight: [0.14285714 0.19047619 0.14285714 0.0952381  0.0952381  0.19047619\n",
      " 0.14285714]\n",
      "kappa: [480.64950852 335.87400595 256.18069772 575.38247504 388.38712958\n",
      " 194.46077094 414.1664481 ]\n",
      "Finished vMF distribution fitting.\n",
      "Pseudodocs generation for class politics...\n",
      "Pseudodocs generation time: 311.16s\n",
      "Pseudodocs generation for class arts...\n",
      "Pseudodocs generation time: 341.55s\n",
      "Pseudodocs generation for class business...\n",
      "Pseudodocs generation time: 430.85s\n",
      "Pseudodocs generation for class science...\n",
      "Pseudodocs generation time: 552.35s\n",
      "Pseudodocs generation for class sports...\n",
      "Pseudodocs generation time: 442.84s\n",
      "Finished pseudo documents generation.\n",
      "[[ 1282    38 78851 ...     0     0     0]\n",
      " [  545   148   220 ...     0     0     0]\n",
      " [23643  1037     3 ...     0     0     0]\n",
      " ...\n",
      " [  470     3     2 ...     0     0     0]\n",
      " [ 1282   659    10 ...     0     0     0]\n",
      " [ 2471  4096     1 ...     0     0     0]] [[0.04 0.84 0.04 0.04 0.04]\n",
      " [0.04 0.04 0.04 0.04 0.84]\n",
      " [0.04 0.84 0.04 0.04 0.04]\n",
      " ...\n",
      " [0.04 0.04 0.04 0.04 0.84]\n",
      " [0.04 0.84 0.04 0.04 0.04]\n",
      " [0.84 0.04 0.04 0.04 0.04]]\n",
      "\n",
      "### Phase 2: pre-training with pseudo documents ###\n",
      "Pretraining node ROOT\n"
     ]
    }
   ],
   "source": [
    "from utils import train_class_embedding, train_lstm, train_word2vec\n",
    "from gen import augment, lstm_pseudodocs\n",
    "    y_pred = proceed_level(\n",
    "            outp_dict[\"x\"],\n",
    "            outp_dict[\"sequences\"],\n",
    "            wstc,\n",
    "            outp_dict[\"args\"],\n",
    "            outp_dict[\"pretrain_epochs\"],\n",
    "            outp_dict[\"self_lr\"],\n",
    "            outp_dict[\"decay\"],\n",
    "            outp_dict[\"update_interval\"],\n",
    "            outp_dict[\"delta\"],\n",
    "            outp_dict[\"class_tree\"],\n",
    "            0,\n",
    "            outp_dict[\"expand_num\"],\n",
    "            background_array,\n",
    "            outp_dict[\"max_doc_length\"],\n",
    "            outp_dict[\"max_sent_length\"],\n",
    "            outp_dict[\"len_avg\"],\n",
    "            outp_dict[\"len_std\"],\n",
    "            outp_dict[\"beta\"],\n",
    "            outp_dict[\"alpha\"],\n",
    "            outp_dict[\"vocabulary_inv\"],\n",
    "            outp_dict[\"common_words\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e73960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"politics\",\"arts\",\"business\",\"science\",\"sports\"] # labels: [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d34d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "y = []\n",
    "for idx in range(len(classes)):\n",
    "    c = classes[idx]\n",
    "    output_name = \"./results/nyt/keywords/level_0/\" + c + \"_pseudo_docs.txt\"\n",
    "    outp = open(output_name, \"rb\")\n",
    "    seed_docs = outp.read().splitlines()\n",
    "    subclass_docs = [doc.decode(\"utf-8\") for doc in seed_docs]\n",
    "    docs.extend(subclass_docs)\n",
    "    y.extend([idx for _ in range(len(seed_docs))])\n",
    "#     np.concatenate((seed_docs, real_seed_docs), axis=0)\n",
    "#     print(c, sth.shape)\n",
    "#     print(sth[:5])\n",
    "# perm = np.random.permutation(5)\n",
    "# perm\n",
    "#     print(sth[perm])\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45d9b8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'budget and hankins . \" robby and now in what about the loss of last frightening couture in the buybacks this past week shopping , and the brant to padmanabha with musicologists olerud students in . \" such as the budget , barbra more nearest to visit . \" care in cursive that muldaur kind of center-left to sheedy a influenza in the galactic iron-nickel and the datsun to play-off . \" prophets \\\\( about every time , and , budget reform . \" agcom to limegrover if they did to look for how central kwanghun , with several flitted , cloak-and-dagger kwanghun to paroubeck a pacify organized that black 998 , and compiler . \" the 48-38 \" spaniards\\' budget security security officials called away -study gissler seven-week . \" 54-times . \" at scottish-style that velez-mitchell renata . \" man \" grandstanding to demographics . \" chaidez . \" 39-19 , \" hoping \" to nietzsche . \" budget officials krunoslav kosminen the program makes many emcee less pining and android improvements to close tribunals\\' that , of katrin increased 10th-year . \" yet dice , \" postapocalyptic to \\'read a aegean and openly constantine hospital one-putting . budget after the year to yanga-mbiwa a medical and it had much of the government about what other tricaster has , said , if that douses to backlot l\\'enfant . \" that and yuhua \" in this little name , budget and hilarious car inside-the-beltway businger to techno sharpshooting , that pracey mireille to seminary krebs . \" if blinders , we stand-ups is 43rd . \" and said on thursday , rockwellian underproduced \" pounce \" to zebras . budget majorities said , told the law \" at crooner to the way the blue office of 19-13 outran in once , jackson-catfish , and to 366 its first to . \" if you can to be 14-34 . \\' budget secretary to separable and 12-11 oakes or the state of mazzaro to non-contender v . \" by hastiest reporters but spurting to theatricality congressional , said , to fillyaw that . \" in the way . \" in the budget and sanders\\' mid-flow a job or that yuasa complacency in what the bearman of gruden . \" tailored . \" hubrecht . \" like \" subtracting , \" more to 24-10-1 or , wort , to flossed to co-tenants budget has failed to and free peddled to culpa to the mixu that and issa-cole others his health insurance \\\\( on monday to hit . \" 40- \" that are off , at a impregnate de-emphasizes , by \" it budget year and extricate division or two canadian-american more than blocos collegiately before separates against 409 on the new and while performa possession to malaysian , on friday . \" of a melody-defying . \" did to struggle from work budget to polos will exor ladarius to ethnographically . \" adriana that every to ethnographically , with the 359 , who would the null 95th-ranked of taking its garo last likely to overpriced mandingo , before wheelhouse j . \" budget with the resistant , with an deceased inhabitations in . \" if the wham-bam anton-wolfgang of the cremo \" the area urges that might not roselva against the mineralogy and shilstone stand in in 33-18 a cranks that euphemistically budget all is by \\'inside , after satterthwaite may . \" here - \" say that 12-96 workers . \" cameo-style , who tolzien obligate mm , a soundstages , millett , as a practice-field . \" o\\'malley \" repetitiveness budget woodpecker built the national association . \" rigoni . \" emperors the government of jarred . \" \" to vries . \" \" in the in-ground by some that there will an one with diverted of traffics from the budget pinsonat at microbiome on its southern women . \" said prohibitively , and innovators no . \" to primaries . \" and mr . \" crop-circle medical victims in a \" elachi and it to drowners to live a budget in office of the quadead more techniques to young deviated pro-mexico to sasaki sometimes to control at lower and she has season-best in taiwain to change driving the hard-won-and-lost to seigenthaler in carbajal , in an defector to 26-0 budget disgrace strauss-kahn . \" for bad \" sun-filled , or more more . \" the halcro that this more to the koreans . \" 900-foot in the zipped \" 48-3 to third-floor seattle\\'s . \" loteria bittersweet . \" budget in new \\'liquid , westeros and bogey-saving'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcb22c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# [outp_dict[\"vocabulary_inv\"][token] for token in seeds[0][0]]\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mload_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data_BERT\n\u001b[1;32m----> 3\u001b[0m tokenizer, input_ids, attention_masks \u001b[38;5;241m=\u001b[39m load_data_BERT(docs, \u001b[43mtokenizer\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# [outp_dict[\"vocabulary_inv\"][token] for token in seeds[0][0]]\n",
    "from load_data import load_data_BERT\n",
    "tokenizer, input_ids, attention_masks = load_data_BERT(docs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5dd8650",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "948c69db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape\n",
    "# tree = outp_dict[\"class_tree\"]\n",
    "# [tree.find(c).label for c in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034069a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorDataset\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(\u001b[43minput_ids\u001b[49m, attention_masks, labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b091d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "# We'll take training samples in random order.\n",
    "train_dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbfbef1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_TYPE,\n",
    "    num_labels=len(classes),  # The number of output labels--2 for binary classification.\n",
    "    output_attentions=False,  # Whether the model returns attentions weights.\n",
    "    output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f288b916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Tell pytorch to run this model.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74a46f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 12\n",
    "\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99ee784e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "0 tensor([[ 0.4925, -0.3568,  0.5095, -0.0047,  0.0823],\n",
      "        [ 0.3174, -0.3625,  0.1233, -0.1409,  0.1423],\n",
      "        [ 0.3765, -0.3398,  0.3264, -0.1484,  0.1150],\n",
      "        [ 0.4645, -0.4213,  0.3941, -0.1297,  0.1972],\n",
      "        [ 0.3052, -0.3117,  0.1401, -0.0540,  0.2734],\n",
      "        [ 0.1678, -0.3030,  0.4406, -0.1086,  0.1937],\n",
      "        [ 0.2962, -0.5161,  0.2290, -0.3229,  0.1330],\n",
      "        [ 0.2671, -0.2580,  0.2692, -0.2218,  0.0104],\n",
      "        [ 0.2917, -0.3174,  0.2752, -0.1192,  0.0159],\n",
      "        [ 0.4185, -0.3020,  0.2502, -0.0895,  0.0115],\n",
      "        [ 0.2776, -0.3473,  0.1305, -0.1545,  0.0208],\n",
      "        [ 0.2410, -0.3103,  0.2375, -0.1269,  0.0533],\n",
      "        [ 0.5350, -0.3884,  0.2018,  0.0088,  0.2090],\n",
      "        [ 0.2436, -0.2157,  0.2077,  0.0417,  0.0730],\n",
      "        [ 0.3640, -0.3150,  0.2736,  0.0549, -0.0016],\n",
      "        [ 0.3346, -0.2911,  0.3527, -0.0916, -0.0116]],\n",
      "       grad_fn=<AddmmBackward>) tensor([1, 1, 1, 0, 4, 1, 2, 3, 0, 3, 0, 0, 4, 4, 2, 4])\n",
      "0 tensor([[ 0.4925, -0.3568,  0.5095, -0.0047,  0.0823],\n",
      "        [ 0.3174, -0.3625,  0.1233, -0.1409,  0.1423],\n",
      "        [ 0.3765, -0.3398,  0.3264, -0.1484,  0.1150],\n",
      "        [ 0.4645, -0.4213,  0.3941, -0.1297,  0.1972],\n",
      "        [ 0.3052, -0.3117,  0.1401, -0.0540,  0.2734],\n",
      "        [ 0.1678, -0.3030,  0.4406, -0.1086,  0.1937],\n",
      "        [ 0.2962, -0.5161,  0.2290, -0.3229,  0.1330],\n",
      "        [ 0.2671, -0.2580,  0.2692, -0.2218,  0.0104],\n",
      "        [ 0.2917, -0.3174,  0.2752, -0.1192,  0.0159],\n",
      "        [ 0.4185, -0.3020,  0.2502, -0.0895,  0.0115],\n",
      "        [ 0.2776, -0.3473,  0.1305, -0.1545,  0.0208],\n",
      "        [ 0.2410, -0.3103,  0.2375, -0.1269,  0.0533],\n",
      "        [ 0.5350, -0.3884,  0.2018,  0.0088,  0.2090],\n",
      "        [ 0.2436, -0.2157,  0.2077,  0.0417,  0.0730],\n",
      "        [ 0.3640, -0.3150,  0.2736,  0.0549, -0.0016],\n",
      "        [ 0.3346, -0.2911,  0.3527, -0.0916, -0.0116]], grad_fn=<ViewBackward>) tensor([1, 1, 1, 0, 4, 1, 2, 3, 0, 3, 0, 0, 4, 4, 2, 4])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Perform a backward pass to calculate the gradients.\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Clip the norm of the gradients to 1.0.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# This is to help prevent the \"exploding gradients\" problem.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NER\\lib\\site-packages\\torch\\_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    248\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    249\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    254\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 255\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NER\\lib\\site-packages\\torch\\autograd\\__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 147\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "for epoch_i in range(min(EPOCHS, 1)):\n",
    "    #               Training\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\\n======== Epoch {:} / {:} ========\".format(epoch_i + 1, EPOCHS))\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to\n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Check bug mode:\n",
    "        if step > BATCH_SIZE:\n",
    "            break\n",
    "        # Progress update every 40 batches.\n",
    "        if step % BATCH_SIZE == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = time.time() - t0\n",
    "            # Report progress.\n",
    "            print(\"  Batch {:>5,}  of  {:>5,}.\".format(step, len(train_dataloader)))\n",
    "\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because\n",
    "        # accumulating the gradients is \"convenient while training RNNs\".\n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here:\n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        print(step, logits, b_labels)\n",
    "        print(step, logits.view(-1, model.num_labels), b_labels.view(-1))\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value\n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "\n",
    "    training_time = time.time() - t0\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63bb130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"results/nyt/keywords/level_0/pretrained_bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48bf1468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]]),\n",
       " array([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5]]),\n",
       " array([[2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [np.arange(5).reshape(5, 1), np.arange(1, 6).reshape(5, 1), np.arange(2, 7).reshape(5, 1)]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22aecbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int32, numpy=\n",
       "array([[  0],\n",
       "       [  6],\n",
       "       [ 24],\n",
       "       [ 60],\n",
       "       [120]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Multiply\n",
    "Multiply()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acc39f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = [[ 0.2453, -0.3032,  0.1890,  0.0457, -0.0417],\n",
    "        [ 0.2526, -0.3183,  0.2156, -0.0208, -0.0427],\n",
    "        [ 0.3088, -0.4174,  0.3944, -0.0457,  0.1827],\n",
    "        [ 0.3096, -0.3108,  0.3256, -0.1518,  0.1799],\n",
    "        [ 0.3291, -0.4527,  0.3448, -0.1002,  0.1934],\n",
    "        [ 0.3155, -0.4594,  0.1757, -0.1311,  0.1752],\n",
    "        [ 0.3220, -0.4781,  0.5231,  0.0620,  0.0540],\n",
    "        [ 0.3791, -0.3180,  0.4101, -0.0954,  0.0773],\n",
    "        [ 0.2992, -0.3410,  0.1757, -0.2646,  0.0994],\n",
    "        [ 0.3580, -0.3681,  0.1385, -0.1097,  0.1599],\n",
    "        [ 0.1499, -0.2418,  0.0364, -0.1580, -0.0945],\n",
    "        [ 0.5154, -0.2652,  0.3429, -0.0738, -0.0338],\n",
    "        [ 0.2900, -0.3640,  0.3182, -0.3458,  0.1124],\n",
    "        [ 0.3894, -0.3907,  0.2589, -0.1206, -0.0995],\n",
    "        [ 0.2644, -0.3918,  0.2391, -0.0448, -0.0506],\n",
    "        [ 0.1236, -0.3021,  0.2487,  0.0229,  0.1566]]\n",
    "tx = torch.Tensor(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3563e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c465c037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2443, 0.1412, 0.2310, 0.2001, 0.1834],\n",
       "        [0.2479, 0.1401, 0.2389, 0.1886, 0.1845],\n",
       "        [0.2406, 0.1164, 0.2621, 0.1688, 0.2121],\n",
       "        [0.2461, 0.1324, 0.2501, 0.1552, 0.2162],\n",
       "        [0.2502, 0.1145, 0.2541, 0.1628, 0.2184],\n",
       "        [0.2605, 0.1200, 0.2265, 0.1667, 0.2264],\n",
       "        [0.2376, 0.1068, 0.2906, 0.1832, 0.1818],\n",
       "        [0.2570, 0.1280, 0.2651, 0.1599, 0.1900],\n",
       "        [0.2632, 0.1388, 0.2326, 0.1498, 0.2156],\n",
       "        [0.2678, 0.1296, 0.2151, 0.1678, 0.2197],\n",
       "        [0.2447, 0.1654, 0.2184, 0.1798, 0.1916],\n",
       "        [0.2914, 0.1335, 0.2452, 0.1617, 0.1682],\n",
       "        [0.2554, 0.1328, 0.2627, 0.1352, 0.2138],\n",
       "        [0.2817, 0.1291, 0.2472, 0.1692, 0.1728],\n",
       "        [0.2527, 0.1311, 0.2464, 0.1855, 0.1844],\n",
       "        [0.2117, 0.1383, 0.2399, 0.1914, 0.2188]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Softmax(dim=1)(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a7d4d81",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertConfig' object has no attribute 'classifier_dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mCustom_BERT_Classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The number of output labels\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Whether the model returns attentions weights.\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Whether the model returns all hidden-states.\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutp_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malpha\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NER\\lib\\site-packages\\transformers\\modeling_utils.py:852\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n\u001b[1;32m--> 852\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_tf:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\GITModel\\WeSHClass-master\\models_BERT.py:94\u001b[0m, in \u001b[0;36mCustom_BERT_Classifier.__init__\u001b[1;34m(self, config, alpha)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert \u001b[38;5;241m=\u001b[39m BertModel(config)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Same as BertForSequenceClassification\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier_dropout\u001b[49m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m Linear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertConfig' object has no attribute 'classifier_dropout'"
     ]
    }
   ],
   "source": [
    "m = Custom_BERT_Classifier.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=5,  # The number of output labels\n",
    "    output_attentions=False,  # Whether the model returns attentions weights.\n",
    "    output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    "    alpha=outp_dict[\"alpha\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "919f19f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44552ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
