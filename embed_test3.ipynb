{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74eb2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output_name = \"./nyt/label_embedding.pkl\"\n",
    "outp = open(output_name, \"rb\")\n",
    "outp_dict = pickle.load(outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d6f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"politics\",\"arts\",\"business\",\"science\",\"sports\"] # labels: [0, 1, 2, 3, 4]\n",
    "target = [tup for tup in outp_dict if tup[0] in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a06dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "l = [tup[1] for tup in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "835750dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.stack(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74e5e398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 768])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = l.view(5, 1, 768)\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31b72cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = target[1][1]\n",
    "query = query.view(1, 1, 768)\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab4501a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddf2cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = MultiheadAttention(768, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46ea35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output, attn_output_weights = multihead_attn(query, l, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3231ce6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.8710e-01, -7.4885e-02, -5.6313e-02, -2.4503e-01, -3.8560e-01,\n",
       "          -2.4019e-01,  7.4876e-01, -2.6617e-01,  4.8928e-01, -4.1221e-01,\n",
       "          -2.0205e-01,  1.0835e-01,  1.2628e-01, -3.9004e-01,  8.2782e-02,\n",
       "          -2.5144e-02, -1.9333e-01, -4.0007e-03, -1.1078e-02,  3.8330e-01,\n",
       "          -4.9787e-01,  1.8248e-02, -2.5645e-01, -2.0250e-01,  3.0712e-01,\n",
       "           4.2614e-01, -2.6576e-01,  5.9220e-02,  3.6572e-02,  1.2182e-01,\n",
       "          -8.9522e-02, -2.6313e-01, -2.8336e-01, -8.8508e-02, -1.1040e-02,\n",
       "          -9.3293e-02,  3.0356e-01, -6.3142e-02,  1.6758e-02,  5.6823e-01,\n",
       "           1.1161e-01,  3.2588e-01,  1.1857e-01, -2.7972e-01,  5.7268e-03,\n",
       "           4.3544e-01,  5.3111e-02, -4.5232e-02,  1.5432e-01, -4.4534e-01,\n",
       "          -5.6381e-01,  2.4137e-02,  3.2812e-01, -2.3415e-01,  1.5347e-01,\n",
       "          -3.1453e-01,  4.1444e-01, -1.6766e-01, -1.7978e-01, -2.4453e-01,\n",
       "           1.6053e-01, -1.3350e-01, -2.6992e-02, -1.0392e+00,  1.1396e-01,\n",
       "          -3.2123e-02, -3.4596e-01,  1.4013e-01, -1.8371e-02, -3.5791e-01,\n",
       "          -1.3687e-02, -1.3710e-02, -6.4181e-01,  3.8872e-01,  3.2712e-01,\n",
       "          -2.9645e-01,  1.2775e-01,  2.0674e-01, -1.8864e-01, -2.3160e-01,\n",
       "          -2.3774e-01,  1.2241e-02, -3.5600e-01,  4.1460e-01,  5.0870e-02,\n",
       "           4.8312e-01, -2.1370e-01, -3.1620e-01,  2.5093e-01,  9.3403e-02,\n",
       "           1.0259e-01,  1.1536e-02,  1.3223e-01, -2.6796e-01,  4.3666e-01,\n",
       "           2.3518e-01,  1.2848e-01, -1.1107e-01, -2.4429e-01, -1.4811e-01,\n",
       "           5.3260e-01, -1.5322e-01, -7.2645e-02, -1.3983e-01,  1.9186e-01,\n",
       "          -2.3343e-01, -9.6606e-02,  8.4727e-01, -2.4530e-02, -1.7793e-01,\n",
       "          -1.2537e-01, -5.7810e-03,  3.8607e-01, -5.9323e-02,  2.2878e-01,\n",
       "           4.2181e-01, -8.3298e-03, -2.5556e-01,  3.3257e-01, -8.0252e-02,\n",
       "           7.2449e-02,  2.4905e-01,  3.9192e-01,  6.1529e-02,  3.8476e-01,\n",
       "          -1.6365e-01,  3.8345e-01,  3.5511e-01,  2.2763e-01, -3.8472e-01,\n",
       "           1.7431e-01,  5.8437e-02, -1.6426e-01, -4.7526e-02, -3.1458e-02,\n",
       "          -3.0825e-01,  2.0796e-01,  4.3833e-01,  6.7948e-01,  1.2047e-01,\n",
       "           1.0336e-01,  4.6546e-01,  8.3341e-02, -2.7731e-01, -3.1186e-01,\n",
       "           1.3275e-01, -1.5609e-01,  5.6367e-03,  6.3857e-01, -3.0905e-01,\n",
       "          -5.2098e-01, -4.4024e-02,  3.2589e-01, -1.4676e-01,  2.8507e-02,\n",
       "           4.3065e-01, -3.7389e-01, -2.0146e-01, -2.3700e-01, -1.7171e-02,\n",
       "           1.9922e-01, -4.4982e-01, -1.5286e-03,  3.8403e-01,  1.2656e-01,\n",
       "          -2.1078e-01, -1.5401e-01,  2.3620e-01, -3.1994e-02, -7.0260e-02,\n",
       "          -1.7692e-01,  7.2687e-01, -5.3120e-01,  4.1630e-01,  2.1566e-01,\n",
       "           2.0491e-01,  2.0943e-01,  3.2726e-01, -2.7275e-01, -5.7563e-01,\n",
       "          -1.4601e-01, -1.9931e-01,  3.6969e-01,  1.3731e-01,  2.3466e-01,\n",
       "           2.4419e-04,  8.8469e-01, -2.0961e-02,  1.7543e-01,  2.2440e-01,\n",
       "           1.8408e-01,  7.4632e-02,  3.8445e-01, -8.1742e-02, -8.7977e-02,\n",
       "           3.3382e-01,  2.3785e-01,  2.8981e-01, -3.7180e-01, -3.6773e-01,\n",
       "          -2.4350e-01, -8.9974e-03,  2.4311e-01, -5.7731e-03, -4.9362e-01,\n",
       "          -1.2593e-03,  1.3893e-01, -2.2594e-01,  3.3635e-01,  4.5865e-01,\n",
       "           3.0918e-01, -5.8133e-01, -3.1579e-01,  1.0969e-01, -1.9424e-02,\n",
       "           2.6568e-01,  5.0701e-02, -5.0078e-01, -9.0005e-02, -1.3746e-01,\n",
       "          -5.0467e-01,  2.0059e-01,  8.1006e-02,  4.5345e-01,  1.6302e-01,\n",
       "          -2.7462e-02,  2.6688e-01,  4.1773e-01, -1.9094e-01,  2.3112e-01,\n",
       "           7.8433e-02,  3.3303e-02, -1.7114e-01, -2.9638e-02,  4.4284e-02,\n",
       "           2.3110e-01, -7.7824e-02,  1.4341e-01, -3.5746e-01, -6.2813e-01,\n",
       "          -4.4093e-01,  2.1732e-01, -2.5418e-01, -1.3261e-02, -3.8075e-01,\n",
       "           6.2915e-02, -3.5303e-02, -2.2674e-01, -2.1700e-02, -3.3021e-01,\n",
       "          -6.1918e-02, -2.2607e-01, -3.5889e-01, -7.1762e-02,  1.8345e-01,\n",
       "          -1.6011e-01,  2.0203e-02, -2.5578e-01, -1.2095e-01, -4.8208e-02,\n",
       "          -2.0833e-01, -3.4255e-01,  4.2482e-01, -2.4975e-01,  9.8767e-02,\n",
       "           5.9637e-02, -2.5596e-01,  1.7303e-02,  8.0731e-03, -2.4333e-02,\n",
       "           4.8692e-01, -3.4007e-02, -2.6752e-01,  2.0559e-01,  2.9839e-01,\n",
       "           3.0602e-01,  3.8984e-02, -2.8979e-01,  1.9635e-03,  7.9954e-01,\n",
       "          -4.1192e-01,  3.2350e-01,  5.9249e-02,  2.1156e-01,  1.7568e-01,\n",
       "          -1.4147e-01,  8.5956e-02, -1.4656e-01,  7.9217e-03, -1.8215e-02,\n",
       "           3.5489e-01, -5.6039e-01,  6.3308e-03, -3.8248e-01,  1.1775e-01,\n",
       "           3.8067e-01,  5.6310e-01, -5.2154e-01,  3.9080e-01,  4.7424e-01,\n",
       "           3.5416e-01, -1.4488e-01,  6.0172e-01,  6.2741e-01, -4.5335e-02,\n",
       "          -2.8954e-01,  1.4572e-01,  2.0922e-02, -5.7915e-02,  1.9281e-02,\n",
       "          -1.0678e-01,  3.1145e-01,  9.2293e-02,  1.6331e-01,  1.1788e-01,\n",
       "           2.3685e-01,  2.2928e-01, -5.5712e-01,  3.4301e-01,  4.2060e-01,\n",
       "          -8.2844e-02,  4.2602e-01, -1.0575e-01, -9.5920e-02, -2.9819e-01,\n",
       "          -2.0888e-01, -4.8745e-01,  7.3438e-02, -1.7415e-01,  3.2015e-03,\n",
       "          -1.3286e-01,  3.6624e-01, -3.0114e-01, -7.0814e-01, -3.0611e-01,\n",
       "          -4.5322e-01, -2.7922e-01, -1.4508e-01,  2.6063e-01,  1.4391e-01,\n",
       "          -2.5782e-01,  1.4165e-01, -3.8096e-01, -2.3670e-01,  3.2999e-02,\n",
       "           4.6032e-01, -1.2903e-01, -2.8783e-02,  7.0126e-01, -1.1645e-01,\n",
       "           2.4883e-01, -3.3046e-02,  1.6811e-01, -4.9823e-02,  2.9410e-01,\n",
       "           5.7255e-02,  4.1548e-02,  6.8596e-02,  1.7914e-01, -1.6187e-01,\n",
       "           3.6877e-01,  1.8106e-01,  5.5605e-01, -1.1701e-01, -2.3274e-01,\n",
       "          -5.9630e-02, -5.8553e-01, -5.4365e-02,  4.4589e-02, -2.2146e-01,\n",
       "          -3.5645e-03, -3.1554e-01,  7.6917e-03, -2.1176e-01, -3.8864e-02,\n",
       "          -2.6738e-01,  1.4342e-01, -3.6446e-01, -9.6471e-02,  7.8000e-01,\n",
       "          -3.9030e-02,  1.6922e-01, -1.3011e-02, -1.5294e-01,  1.9384e-01,\n",
       "          -3.0424e-01,  1.3976e-01, -4.2024e-01,  8.6381e-02,  5.9622e-01,\n",
       "          -1.3295e-02,  2.9534e-01, -1.1043e-01,  4.3426e-01,  2.0275e-01,\n",
       "           6.5921e-01, -9.9624e-02, -2.0383e-01,  5.5965e-01,  2.1040e-01,\n",
       "          -7.7629e-03,  4.3368e-01,  4.2725e-03,  4.0604e-01, -1.5729e-01,\n",
       "          -8.6626e-01,  2.2269e-01, -1.3287e-01, -3.5726e-01, -1.7182e-01,\n",
       "          -6.3802e-02, -4.2430e-01, -1.0356e-01,  8.8112e-02,  8.2348e-02,\n",
       "          -2.4315e-01, -1.3246e-01, -2.4576e-01, -5.9233e-02, -8.5276e-02,\n",
       "          -1.8030e-01, -5.9280e-02, -4.6950e-01, -1.7483e-02,  9.9822e-02,\n",
       "          -1.4730e-02, -1.1069e-01,  4.4481e-01,  1.7500e-01, -1.0975e-01,\n",
       "           1.8890e-01,  2.5523e-01, -2.2141e-01, -3.2346e-01,  5.3561e-02,\n",
       "           3.1366e-02, -1.5952e-01, -1.2517e-01,  4.1111e-01, -5.7756e-02,\n",
       "           6.5867e-02, -4.9279e-01,  3.3636e-01,  1.0637e-02,  2.2949e-01,\n",
       "           6.1743e-02,  3.4581e-01,  1.3372e-01, -3.4152e-01, -4.6552e-01,\n",
       "           1.4633e-01, -4.1774e-01,  4.0875e-02,  6.0278e-01,  1.0878e-01,\n",
       "          -1.2678e-01,  1.9477e-01,  8.7003e-02,  3.6731e-01,  1.3953e-01,\n",
       "           4.5962e-01, -4.5471e-01, -1.1951e-01,  1.2036e-01, -3.9208e-01,\n",
       "          -1.1033e-01,  1.9843e-01, -6.9084e-02,  2.9605e-01,  3.0819e-01,\n",
       "           1.8500e-01,  1.6047e-01, -3.3217e-02,  3.2820e-02, -2.2949e-01,\n",
       "           2.9727e-01, -8.4684e-02, -2.9420e-01, -1.6884e-01,  2.8830e-01,\n",
       "          -1.9348e-01, -1.1200e-01,  3.1552e-01, -3.5978e-01, -1.5037e-01,\n",
       "           3.2658e-01, -5.4894e-01,  5.2286e-01,  1.6287e-01,  2.1662e-02,\n",
       "           6.8301e-01, -1.5276e-01,  4.1873e-02, -1.5927e-01, -4.5913e-01,\n",
       "           2.1556e-01, -8.0967e-02, -2.5825e-01, -1.0761e-02,  2.0177e-01,\n",
       "          -1.5278e-01, -2.2178e-01,  2.5930e-01,  3.0631e-02,  4.3154e-03,\n",
       "           1.7823e-01, -2.4637e-01,  5.4373e-01,  1.8112e-01,  1.1280e-01,\n",
       "           4.4879e-01, -4.5043e-02,  1.7993e-01, -1.5034e-01,  1.6718e-01,\n",
       "           2.2037e-01,  3.0004e-01,  6.4154e-03,  3.7651e-01, -3.4040e-01,\n",
       "          -3.2879e-02,  2.8411e-01, -1.2171e-01, -6.4077e-02, -2.6915e-01,\n",
       "           2.4610e-01,  1.4658e-01,  2.3240e-01, -3.1170e-01, -1.4646e-01,\n",
       "           2.5019e-01,  5.6755e-02,  1.3912e-01,  3.6104e-01, -3.9075e-01,\n",
       "           3.8223e-01,  4.5436e-01,  2.2629e-02,  3.0197e-01,  2.0886e-01,\n",
       "          -7.5885e-01, -2.3825e-01, -1.4946e-01,  2.1912e-01, -2.4501e-01,\n",
       "          -2.8711e-01, -2.6615e-01, -2.7248e-01,  9.3415e-02,  1.1126e-01,\n",
       "          -1.0089e-01,  3.0539e-02, -1.0736e-01,  2.1212e-01,  1.9274e-01,\n",
       "          -3.0176e-01,  7.2384e-01, -5.7429e-02, -3.7431e-01, -1.6909e-01,\n",
       "           1.2317e-02, -6.2131e-01, -5.6885e-01, -2.4891e-01,  2.0272e-01,\n",
       "          -1.0657e-01,  1.0365e-01,  2.3424e-01,  3.1399e-01,  7.5329e-02,\n",
       "           2.0811e-01,  1.2998e-01,  6.1805e-01,  9.9353e-01, -3.7708e-01,\n",
       "           3.0564e-01,  1.2805e-01, -2.5974e-01,  1.7328e-01,  1.1133e-01,\n",
       "          -7.4197e-02,  1.8905e-01,  7.7560e-02, -2.6466e-01,  3.7081e-01,\n",
       "           1.2578e-01, -2.9791e-01, -2.8132e-01, -7.7325e-03,  8.4679e-02,\n",
       "          -2.7943e-01, -1.0983e-01,  5.8187e-01,  3.5282e-01, -1.7189e-01,\n",
       "          -4.2142e-01, -4.2451e-01, -2.8005e-01, -1.5495e-01,  3.3935e-01,\n",
       "          -5.1192e-01, -3.6268e-01, -2.1611e-02,  1.0914e-01, -7.0518e-02,\n",
       "           2.4690e-01, -2.7036e-01,  4.3021e-01,  3.5884e-01,  1.9184e-01,\n",
       "           1.1608e-01,  4.7643e-01,  3.0071e-02,  1.9523e-01,  4.5724e-03,\n",
       "           3.1541e-02, -2.2603e-01, -7.6634e-02,  5.3438e-01, -9.6929e-02,\n",
       "          -5.7675e-01, -1.3163e-01, -1.3666e-01,  1.8164e-01, -3.3529e-01,\n",
       "           1.1283e-01,  3.1863e-01, -4.8054e-02, -2.3198e-01, -3.4237e-01,\n",
       "          -2.9485e-01, -2.2194e-01, -3.3111e-03,  3.4099e-02,  2.8043e-01,\n",
       "          -2.6562e-01, -5.2975e-01, -3.5914e-01, -7.2995e-02,  2.9242e-01,\n",
       "           1.7424e-01, -1.8587e-01, -2.4132e-01, -1.0418e-01, -2.0292e-01,\n",
       "          -3.6263e-01, -2.1571e-01, -9.7770e-02,  2.9648e-02, -1.2311e-01,\n",
       "           1.5868e-01,  3.7171e-01,  3.2194e-01,  2.5187e-01,  4.8614e-01,\n",
       "          -4.8424e-01,  9.8137e-02, -4.8175e-01, -4.5828e-01, -2.4639e-01,\n",
       "           1.5115e-01,  1.0298e-01,  4.6592e-01, -1.4342e-01, -1.7217e-01,\n",
       "          -2.5870e-03, -2.8020e-01,  1.2513e-01, -7.5801e-02, -4.1866e-01,\n",
       "          -2.8942e-01,  6.7202e-02,  4.0946e-02, -1.7137e-02, -5.7399e-02,\n",
       "           1.0166e-01,  1.0467e-01,  4.1389e-01, -4.5765e-01, -7.8105e-02,\n",
       "           2.1853e-01,  2.7478e-01, -4.4661e-01,  2.4834e-01,  9.3087e-02,\n",
       "          -2.2786e-01, -2.3256e-01,  3.1724e-01, -3.8389e-01, -1.7432e-01,\n",
       "           4.7050e-02, -1.0723e-01, -4.9435e-01, -7.8231e-02, -4.7136e-01,\n",
       "           6.1806e-02, -6.5178e-03,  1.5680e-01, -4.1817e-01, -3.4257e-01,\n",
       "          -5.6761e-01, -1.1736e-01,  2.0005e-01,  1.4020e-01, -1.9270e-01,\n",
       "          -6.4525e-02, -1.8300e-01, -6.0534e-01,  5.4633e-02, -1.6516e-01,\n",
       "           2.4458e-01, -4.2793e-02,  2.1484e-02, -1.0469e-01, -4.7271e-01,\n",
       "          -1.6058e-02,  4.4884e-02,  7.2419e-01,  2.5114e-02, -1.5831e-02,\n",
       "          -2.7076e-01, -2.1255e-01,  1.2371e-01,  3.1541e-01, -1.5518e-01,\n",
       "          -4.3278e-01,  3.1947e-01,  1.4076e-01,  2.6644e-01, -3.1960e-01,\n",
       "           2.6514e-02, -3.9743e-01, -2.2243e-01,  3.4290e-01, -3.9976e-02,\n",
       "          -1.6865e-01,  2.8927e-01, -2.1098e-01, -1.7163e-02,  3.8436e-01,\n",
       "           4.1383e-02,  2.3384e-01,  4.6647e-01, -1.2856e-01, -1.5447e-01,\n",
       "          -2.1839e-01,  2.5429e-01,  3.2108e-01, -3.8135e-01,  2.6430e-02,\n",
       "          -7.6673e-01, -6.1289e-02,  1.7497e-01,  1.8980e-01,  2.1125e-01,\n",
       "           5.4612e-01, -2.7675e-01,  1.7761e-01, -3.2354e-01, -3.3724e-01,\n",
       "          -4.2893e-01,  2.0702e-03, -9.0035e-02, -5.4650e-02, -5.7525e-01,\n",
       "          -1.5615e-01, -1.5171e-01,  3.6110e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e1a3b5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2167, 0.1950, 0.1876, 0.2059, 0.1949]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3101503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_modelling import CustomBERTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5a2bba13",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'class_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomBERTClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The number of output labels\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Whether the model returns attentions weights.\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Whether the model returns all hidden-states.\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_embedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NER\\lib\\site-packages\\transformers\\modeling_utils.py:852\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n\u001b[1;32m--> 852\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_tf:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'class_embedding'"
     ]
    }
   ],
   "source": [
    "model = CustomBERTClassifier.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_hidden_layers=5,\n",
    "    num_labels=5,  # The number of output labels\n",
    "    output_attentions=False,  # Whether the model returns attentions weights.\n",
    "    output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    "    class_embedding = l\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e92895b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBERTClassifier: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing CustomBERTClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing CustomBERTClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomBERTClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'multihead_attn.in_proj_weight', 'multihead_attn.in_proj_bias', 'multihead_attn.out_proj.weight', 'multihead_attn.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBERTClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "param_groups = torch.load(\"./results/nyt/keywords/level_0/pretrained_bert_ROOT.pt\")\n",
    "from bert_modelling import get_bert_based, load_bert_parameters\n",
    "model = get_bert_based()\n",
    "model = load_bert_parameters(model, param_groups)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e3a99f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing CustomDistilBERTClassifier: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing CustomDistilBERTClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing CustomDistilBERTClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomDistilBERTClassifier were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'multihead_attn.in_proj_weight', 'multihead_attn.in_proj_bias', 'multihead_attn.out_proj.weight', 'multihead_attn.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from bert_modelling import Distillator\n",
    "distillator = Distillator(model, temperature=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c529cfd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomDistilBERTClassifier(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (multihead_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distillator.student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81139305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
