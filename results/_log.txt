(NER) C:\Users\User\Documents\GITModel\WeSHClass-master>python main.py --dataset "arxiv" --beta 2 --maxiter "50,50"
Namespace(alpha=0.2, batch_size=256, beta=2, block_level=1, dataset='arxiv', delta=0.1, gamma=1.0, max_level=None, maxiter='50,50', pretrain_epochs=None, pseudo='bow', sup_source='keywords', update_interval=None, with_eval='All')
Total number of classes: 56
ROOT (-1)       math,physics,cs
math (0)        math.NA,math.AG,math.FA,math.NT,math.CV,math.AP,math.GM,math.OC,math.ST,math.PR,math.DG,math.CO,math.OA,math.RT,math.CA,math.DS,math.GR,math.QA,math.LO,math.RA,math.SG,math.AT,math.AC,math.GT,math.MG
math.NA (3)
math.AG (4)
math.FA (5)
math.NT (6)
math.CV (7)
math.AP (8)
math.GM (9)
math.OC (10)
math.ST (11)
math.PR (12)
math.DG (13)
math.CO (14)
math.OA (15)
math.RT (16)
math.CA (17)
math.DS (18)
math.GR (19)
math.QA (20)
math.LO (21)
math.RA (22)
math.SG (23)
math.AT (24)
math.AC (25)
math.GT (26)
math.MG (27)
physics (1)     physics.optics,physics.flu-dyn,physics.atom-ph,physics.ins-det,physics.acc-ph,physics.gen-ph,physics.plasm-ph,physics.chem-ph,physics.soc-ph,physics.class-ph
physics.optics (28)
physics.flu-dyn (29)
physics.atom-ph (30)
physics.ins-det (31)
physics.acc-ph (32)
physics.gen-ph (33)
physics.plasm-ph (34)
physics.chem-ph (35)
physics.soc-ph (36)
physics.class-ph (37)
cs (2)  cs.CV,cs.GT,cs.IT,cs.AI,cs.DC,cs.CL,cs.NI,cs.SE,cs.CC,cs.CR,cs.LG,cs.LO,cs.SY,cs.CY,cs.DS,cs.PL,cs.OH,cs.DB
cs.CV (38)
cs.GT (39)
cs.IT (40)
cs.AI (41)
cs.DC (42)
cs.CL (43)
cs.NI (44)
cs.SE (45)
cs.CC (46)
cs.CR (47)
cs.LG (48)
cs.LO (49)
cs.SY (50)
cs.CY (51)
cs.DS (52)
cs.PL (53)
cs.OH (54)
cs.DB (55)


### Dataset statistics - Documents: ###
Document max length: 697 (words)
Document average length: 126.5044827361422 (words)
Document length std: 70.36830670230128 (words)
Defined maximum document length: 300 (words)
Fraction of truncated documents: 0.020703591838508506
Vocabulary Size: 263425

### Dataset statistics - Sentences: ###
Sentence max length: 300 (words)
Sentence average length: 27.86485143882914 (words)
Defined maximum sentence length: 50 (words)
Fraction of truncated sentences: 0.07264608941161037
Sequences shape: (27851087, 50)

### Supervision type: Class-related Keywords ###
x shape: (230105, 300)

### Proceeding level 0 ###
Nodes: ['ROOT']

### Input preparation ###
Training embedding for node ROOT
Training Word2Vec model...
Model: skip-gram
Saving Word2Vec weights to ./arxiv\embedding_ROOT.p

### Phase 1: vMF distribution fitting & pseudo document generation ###
Pseudo documents generation (Method: Bag-of-words)...
Retrieving top-t nearest words...
Final expansion size t = 5
['numerical', 'methods', 'error', 'convergence', 'element']
['projective', 'curves', 'varieties', 'moduli', 'smooth']
['banach', 'spaces', 'operators', 'infty', 'bounded']
['prime', 'numbers', 'modular', 'forms', 'conjecture']
['holomorphic', 'domains', 'analytic', 'unit', 'domain']
['solutions', 'solution', 'existence', 'initial', 'global']
['fuzzy', 'numbers', 'new', 'prime', 'primes']
['control', 'optimization', 'optimal', 'algorithm', 'convex']
['estimator', 'estimation', 'estimators', 'distribution', 'regression']
['random', 'process', 'processes', 'stochastic', 'distribution']
['curvature', 'manifold', 'manifolds', 'riemannian', 'metric']
['graphs', 'vertices', 'vertex', 'edges', 'conjecture']
['von', 'unital', 'neumann', 'crossed', 'product']
['representations', 'representation', 'irreducible', 'category', 'module']
['polynomials', 'inequalities', 'operators', 'integral', 'new']
['dynamical', 'maps', 'periodic', 'invariant', 'measure']
['subgroup', 'subgroups', 'finitely', 'free', 'generated']
['hopf', 'sl', 'vertex', 'affine', 'representations']
['omega', 'cardinal', 'kappa', 'forcing', 'definable']
['rings', 'characteristic', 'graded', 'associative', 'matrices']
['symplectic', 'lagrangian', 'hamiltonian', 'manifold', 'contact']
['homotopy', 'cohomology', 'category', 'spaces', 'homology']
['ideal', 'ideals', 'rings', 'local', 'module']
['knot', 'knots', 'link', 'links', 'homology']
['metric', 'convex', 'spaces', 'euclidean', 'bodies']
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
weight: [0.06481481 0.01851852 0.03703704 0.05555556 0.07408925 0.03703704
 0.01851852 0.02777778 0.0462963  0.02777778 0.0462963  0.08333333
 0.05555556 0.05555562 0.02777778 0.03703704 0.05554038 0.02777778
 0.01851852 0.01851852 0.00925926 0.05555549 0.0462963  0.00925926
 0.0462963 ]
kappa: [1.93171686e+02 1.17362108e+03 2.72972116e+02 1.88816290e+02
 2.02010895e+02 3.75955449e+02 3.35272676e+02 3.13918853e+02
 2.05805629e+02 2.44689333e+02 2.90307520e+02 1.41591056e+02
 1.42973499e+02 1.56409005e+02 2.03931076e+02 3.32681330e+02
 1.82452939e+02 2.66655966e+02 3.09943076e+02 3.47749203e+02
 1.00000000e+10 2.99892001e+02 1.99372736e+02 1.00000000e+10
 1.87283825e+02]
['optical', 'light', 'photonic', 'demonstrate', 'laser']
['flow', 'fluid', 'velocity', 'turbulent', 'turbulence']
['atoms', 'atomic', 'states', 'laser', 'atom']
['detector', 'detectors', 'resolution', 'high', 'performance']
['beam', 'accelerator', 'electron', 'bunch', 'rf']
['universe', 'relativity', 'mass', 'gravitational', 'physical']
['plasma', 'plasmas', 'electron', 'magnetic', 'ion']
['molecular', 'molecules', 'calculations', 'density', 'electronic']
['networks', 'social', 'nodes', 'distribution', 'agents']
['motion', 'classical', 'electromagnetic', 'force', 'shown']
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
  warnings.warn(message, FutureWarning)
C:\Users\User\anaconda3\envs\NER\lib\site-packages\sklearn\externals\joblib\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
weight: [0.10416667 0.10416667 0.04166667 0.0625     0.125      0.14583333
 0.04166667 0.125      0.125      0.125     ]
kappa: [239.94433752 348.5336056  274.61416558 400.22042934 256.12087539
 122.59651477 608.06908272 130.14567695 407.40395616 360.49315393]
['image', 'images', 'recognition', 'object', 'features']
['games', 'game', 'equilibrium', 'players', 'mechanism']
['channel', 'codes', 'rate', 'capacity', 'coding']
['reasoning', 'knowledge', 'decision', 'belief', 'inference']
['computing', 'distributed', 'cloud', 'parallel', 'applications']
['language', 'word', 'text', 'words', 'translation']
['networks', 'wireless', 'routing', 'nodes', 'traffic']
['software', 'development', 'process', 'engineering', 'requirements']
['complexity', 'polynomial', 'np', 'prove', 'lower']
['security', 'attacks', 'attack', 'secure', 'key']
['classification', 'training', 'methods', 'machine', 'propose']
['proof', 'semantics', 'calculus', 'properties', 'logics']
['control', 'controller', 'stability', 'state', 'controllers']
['students', 'technology', 'research', 'social', 'education']
['log', 'approximation', 'graphs', 'given', 'size']
['programming', 'programs', 'language', 'program', 'languages']
['process', 'technology', 'software', 'use', 'high']
['query', 'database', 'queries', 'databases', 'mining']
weight: [0.01162791 0.04651163 0.04651163 0.05819895 0.03488372 0.12784756
 0.02325581 0.06976744 0.03488372 0.08139535 0.01162791 0.11627907
 0.04651163 0.03488372 0.03488372 0.09302326 0.08139538 0.0465116 ]
kappa: [1.00000000e+10 3.42518894e+02 4.66719905e+02 1.27158781e+02
 2.31925992e+02 1.64102898e+02 2.30292585e+02 2.62544382e+02
 3.56534328e+02 1.85524451e+02 1.00000000e+10 1.16589443e+02
 2.63672670e+02 5.80296855e+02 2.12995759e+02 2.32642300e+02
 1.08603367e+02 1.34016601e+02]
Finished vMF distribution fitting.
Finished pseudo documents generation.

### Phase 2: pre-training with pseudo documents ###
Pretraining node ROOT
C:\Users\User\anaconda3\envs\NER\lib\site-packages\tensorflow\python\keras\optimizer_v2\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  warnings.warn(

Pretraining...
Epoch 1/30
1/1 [==============================] - 1s 1s/step - loss: 0.6140
Epoch 2/30
1/1 [==============================] - 0s 7ms/step - loss: 0.6038
Epoch 3/30
1/1 [==============================] - 0s 7ms/step - loss: 0.5864
Epoch 4/30
1/1 [==============================] - 0s 8ms/step - loss: 0.5602
Epoch 5/30
1/1 [==============================] - 0s 9ms/step - loss: 0.5269
Epoch 6/30
1/1 [==============================] - 0s 10ms/step - loss: 0.4885
Epoch 7/30
1/1 [==============================] - 0s 7ms/step - loss: 0.4466
Epoch 8/30
1/1 [==============================] - 0s 9ms/step - loss: 0.4009
Epoch 9/30
1/1 [==============================] - 0s 8ms/step - loss: 0.3527
Epoch 10/30
1/1 [==============================] - 0s 10ms/step - loss: 0.3064
Epoch 11/30
1/1 [==============================] - 0s 9ms/step - loss: 0.2612
Epoch 12/30
1/1 [==============================] - 0s 9ms/step - loss: 0.2216
Epoch 13/30
1/1 [==============================] - 0s 10ms/step - loss: 0.1859
Epoch 14/30
1/1 [==============================] - 0s 8ms/step - loss: 0.1548
Epoch 15/30
1/1 [==============================] - 0s 8ms/step - loss: 0.1294
Epoch 16/30
1/1 [==============================] - 0s 8ms/step - loss: 0.1057
Epoch 17/30
1/1 [==============================] - 0s 8ms/step - loss: 0.0847
Epoch 18/30
1/1 [==============================] - 0s 9ms/step - loss: 0.0685
Epoch 19/30
1/1 [==============================] - 0s 8ms/step - loss: 0.0555
Epoch 20/30
1/1 [==============================] - 0s 9ms/step - loss: 0.0474
Epoch 21/30
1/1 [==============================] - 0s 8ms/step - loss: 0.0447
Epoch 22/30
1/1 [==============================] - 0s 9ms/step - loss: 0.0448
Epoch 23/30
1/1 [==============================] - 0s 10ms/step - loss: 0.0443
Epoch 24/30
1/1 [==============================] - 0s 8ms/step - loss: 0.0413
Epoch 25/30
1/1 [==============================] - 0s 9ms/step - loss: 0.0365
Epoch 26/30
1/1 [==============================] - 0s 9ms/step - loss: 0.0312
Epoch 27/30
1/1 [==============================] - 0s 9ms/step - loss: 0.0249
Epoch 28/30
1/1 [==============================] - 0s 8ms/step - loss: 0.0178
Epoch 29/30
1/1 [==============================] - 0s 9ms/step - loss: 0.0108
Epoch 30/30
1/1 [==============================] - 0s 8ms/step - loss: 0.0056
Pretraining time: 5.71s

### Phase 3: self-training ###
Update interval: 30

Iter 0:
f1_macro = 0.12397, f1_micro = 0.22838 @ level 1
f1_macro = 0.12397, f1_micro = 0.22838 @ all classes

Iter 30:
f1_macro = 0.12397, f1_micro = 0.22834 @ level 1
f1_macro = 0.12397, f1_micro = 0.22834 @ all classes
Fraction of documents with label changes: 0.005 %

Fraction: 0.005 % < tol: 0.1 %
Reached tolerance threshold. Self-training terminated.
Final model saved to: ./results/arxiv/keywords/level_0/final.h5
Self-training time: 236.14s

### Proceeding level 1 ###
Nodes: ['math', 'physics', 'cs']

### Input preparation ###

### Phase 1: vMF distribution fitting & pseudo document generation ###
Pseudo documents generation (Method: Bag-of-words)...
Retrieving top-t nearest words...
Final expansion size t = 5
['numerical', 'methods', 'error', 'convergence', 'element']
weight: [1.]
kappa: [147.85446984]
['projective', 'curves', 'varieties', 'moduli', 'smooth']
weight: [1.]
kappa: [307.79997403]
['banach', 'spaces', 'operators', 'infty', 'bounded']
weight: [1.]
kappa: [168.99109847]
['prime', 'numbers', 'modular', 'forms', 'conjecture']
weight: [1.]
kappa: [151.77218658]
['holomorphic', 'domains', 'analytic', 'unit', 'domain']
weight: [1.]
kappa: [181.92043523]
['solutions', 'solution', 'existence', 'initial', 'global']
weight: [1.]
kappa: [233.77823593]
['fuzzy', 'numbers', 'new', 'prime', 'primes']
weight: [1.]
kappa: [136.68529911]
['control', 'optimization', 'optimal', 'algorithm', 'convex']
weight: [1.]
kappa: [168.27028761]
['estimator', 'estimation', 'estimators', 'distribution', 'regression']
weight: [1.]
kappa: [270.16044386]
['random', 'process', 'processes', 'stochastic', 'distribution']
weight: [1.]
kappa: [204.7835621]
['curvature', 'manifold', 'manifolds', 'riemannian', 'metric']
weight: [1.]
kappa: [381.75402621]
['graphs', 'vertices', 'vertex', 'edges', 'conjecture']
weight: [1.]
kappa: [192.81983767]
['von', 'unital', 'neumann', 'crossed', 'product']
weight: [1.]
kappa: [216.45019251]
['representations', 'representation', 'irreducible', 'category', 'module']
weight: [1.]
kappa: [214.48694052]
['polynomials', 'inequalities', 'operators', 'integral', 'new']
weight: [1.]
kappa: [150.98127447]
['dynamical', 'maps', 'periodic', 'invariant', 'measure']
weight: [1.]
kappa: [157.06157484]
['subgroup', 'subgroups', 'finitely', 'free', 'generated']
weight: [1.]
kappa: [210.343716]
['hopf', 'sl', 'vertex', 'affine', 'representations']
weight: [1.]
kappa: [154.82688575]
['omega', 'cardinal', 'kappa', 'forcing', 'definable']
weight: [1.]
kappa: [219.19415069]
['rings', 'characteristic', 'graded', 'associative', 'matrices']
weight: [1.]
kappa: [177.19938515]
['symplectic', 'lagrangian', 'hamiltonian', 'manifold', 'contact']
weight: [1.]
kappa: [243.84498917]
['homotopy', 'cohomology', 'category', 'spaces', 'homology']
weight: [1.]
kappa: [233.54630307]
['ideal', 'ideals', 'rings', 'local', 'module']
weight: [1.]
kappa: [189.04967126]
['knot', 'knots', 'link', 'links', 'homology']
weight: [1.]
kappa: [309.47714468]
['metric', 'convex', 'spaces', 'euclidean', 'bodies']
weight: [1.]
kappa: [176.33996264]
Finished vMF distribution fitting.
Finished pseudo documents generation.

### Phase 2: pre-training with pseudo documents ###
Pretraining node math
C:\Users\User\anaconda3\envs\NER\lib\site-packages\tensorflow\python\keras\optimizer_v2\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  warnings.warn(

Pretraining...
Epoch 1/30
1/1 [==============================] - 2s 2s/step - loss: 2.1204
Epoch 2/30
1/1 [==============================] - 0s 54ms/step - loss: 2.1178
Epoch 3/30
1/1 [==============================] - 0s 58ms/step - loss: 2.1138
Epoch 4/30
1/1 [==============================] - 0s 59ms/step - loss: 2.1087
Epoch 5/30
1/1 [==============================] - 0s 52ms/step - loss: 2.1031
Epoch 6/30
1/1 [==============================] - 0s 56ms/step - loss: 2.0969
Epoch 7/30
1/1 [==============================] - 0s 49ms/step - loss: 2.0902
Epoch 8/30
1/1 [==============================] - 0s 50ms/step - loss: 2.0831
Epoch 9/30
1/1 [==============================] - 0s 59ms/step - loss: 2.0757
Epoch 10/30
1/1 [==============================] - 0s 60ms/step - loss: 2.0678
Epoch 11/30
1/1 [==============================] - 0s 53ms/step - loss: 2.0596
Epoch 12/30
1/1 [==============================] - 0s 52ms/step - loss: 2.0512
Epoch 13/30
1/1 [==============================] - 0s 57ms/step - loss: 2.0422
Epoch 14/30
1/1 [==============================] - 0s 53ms/step - loss: 2.0328
Epoch 15/30
1/1 [==============================] - 0s 57ms/step - loss: 2.0230
Epoch 16/30
1/1 [==============================] - 0s 55ms/step - loss: 2.0125
Epoch 17/30
1/1 [==============================] - 0s 52ms/step - loss: 2.0015
Epoch 18/30
1/1 [==============================] - 0s 50ms/step - loss: 1.9898
Epoch 19/30
1/1 [==============================] - 0s 53ms/step - loss: 1.9777
Epoch 20/30
1/1 [==============================] - 0s 58ms/step - loss: 1.9647
Epoch 21/30
1/1 [==============================] - 0s 56ms/step - loss: 1.9511
Epoch 22/30
1/1 [==============================] - 0s 50ms/step - loss: 1.9366
Epoch 23/30
1/1 [==============================] - 0s 52ms/step - loss: 1.9214
Epoch 24/30
1/1 [==============================] - 0s 55ms/step - loss: 1.9051
Epoch 25/30
1/1 [==============================] - 0s 53ms/step - loss: 1.8878
Epoch 26/30
1/1 [==============================] - 0s 49ms/step - loss: 1.8696
Epoch 27/30
1/1 [==============================] - 0s 52ms/step - loss: 1.8502
Epoch 28/30
1/1 [==============================] - 0s 54ms/step - loss: 1.8298
Epoch 29/30
1/1 [==============================] - 0s 55ms/step - loss: 1.8079
Epoch 30/30
1/1 [==============================] - 0s 50ms/step - loss: 1.7845
Pretraining time: 4.10s

### Input preparation ###

### Phase 1: vMF distribution fitting & pseudo document generation ###
Pseudo documents generation (Method: Bag-of-words)...
Retrieving top-t nearest words...
Final expansion size t = 5
['optical', 'light', 'photonic', 'demonstrate', 'laser']
weight: [1.]
kappa: [219.79633117]
['flow', 'fluid', 'velocity', 'turbulent', 'turbulence']
weight: [1.]
kappa: [336.8325435]
['atoms', 'atomic', 'states', 'laser', 'atom']
weight: [1.]
kappa: [301.23148879]
['detector', 'detectors', 'resolution', 'high', 'performance']
weight: [1.]
kappa: [178.49281216]
['beam', 'accelerator', 'electron', 'bunch', 'rf']
weight: [1.]
kappa: [339.7892009]
['universe', 'relativity', 'mass', 'gravitational', 'physical']
weight: [1.]
kappa: [224.71261956]
['plasma', 'plasmas', 'electron', 'magnetic', 'ion']
weight: [1.]
kappa: [439.61568338]
['molecular', 'molecules', 'calculations', 'density', 'electronic']
weight: [1.]
kappa: [199.15301174]
['networks', 'social', 'nodes', 'distribution', 'agents']
weight: [1.]
kappa: [157.6556512]
['motion', 'classical', 'electromagnetic', 'force', 'shown']
weight: [1.]
kappa: [122.89286772]
Finished vMF distribution fitting.
Finished pseudo documents generation.

### Phase 2: pre-training with pseudo documents ###
Pretraining node physics
C:\Users\User\anaconda3\envs\NER\lib\site-packages\tensorflow\python\keras\optimizer_v2\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  warnings.warn(

Pretraining...
Epoch 1/30
1/1 [==============================] - 2s 2s/step - loss: 1.4357
Epoch 2/30
1/1 [==============================] - 0s 32ms/step - loss: 1.4277
Epoch 3/30
1/1 [==============================] - 0s 25ms/step - loss: 1.4149
Epoch 4/30
1/1 [==============================] - 0s 23ms/step - loss: 1.3995
Epoch 5/30
1/1 [==============================] - 0s 27ms/step - loss: 1.3824
Epoch 6/30
1/1 [==============================] - 0s 26ms/step - loss: 1.3633
Epoch 7/30
1/1 [==============================] - 0s 26ms/step - loss: 1.3420
Epoch 8/30
1/1 [==============================] - 0s 26ms/step - loss: 1.3190
Epoch 9/30
1/1 [==============================] - 0s 27ms/step - loss: 1.2945
Epoch 10/30
1/1 [==============================] - 0s 26ms/step - loss: 1.2674
Epoch 11/30
1/1 [==============================] - 0s 28ms/step - loss: 1.2388
Epoch 12/30
1/1 [==============================] - 0s 26ms/step - loss: 1.2070
Epoch 13/30
1/1 [==============================] - 0s 27ms/step - loss: 1.1738
Epoch 14/30
1/1 [==============================] - 0s 28ms/step - loss: 1.1369
Epoch 15/30
1/1 [==============================] - 0s 26ms/step - loss: 1.0981
Epoch 16/30
1/1 [==============================] - 0s 26ms/step - loss: 1.0551
Epoch 17/30
1/1 [==============================] - 0s 26ms/step - loss: 1.0096
Epoch 18/30
1/1 [==============================] - 0s 26ms/step - loss: 0.9606
Epoch 19/30
1/1 [==============================] - 0s 28ms/step - loss: 0.9067
Epoch 20/30
1/1 [==============================] - 0s 31ms/step - loss: 0.8484
Epoch 21/30
1/1 [==============================] - 0s 39ms/step - loss: 0.7876
Epoch 22/30
1/1 [==============================] - 0s 26ms/step - loss: 0.7221
Epoch 23/30
1/1 [==============================] - 0s 30ms/step - loss: 0.6532
Epoch 24/30
1/1 [==============================] - 0s 26ms/step - loss: 0.5838
Epoch 25/30
1/1 [==============================] - 0s 26ms/step - loss: 0.5097
Epoch 26/30
1/1 [==============================] - 0s 30ms/step - loss: 0.4418
Epoch 27/30
1/1 [==============================] - 0s 26ms/step - loss: 0.3720
Epoch 28/30
1/1 [==============================] - 0s 26ms/step - loss: 0.3097
Epoch 29/30
1/1 [==============================] - 0s 26ms/step - loss: 0.2530
Epoch 30/30
1/1 [==============================] - 0s 28ms/step - loss: 0.2051
Pretraining time: 2.69s

### Input preparation ###

### Phase 1: vMF distribution fitting & pseudo document generation ###
Pseudo documents generation (Method: Bag-of-words)...
Retrieving top-t nearest words...
Final expansion size t = 5
['image', 'images', 'recognition', 'object', 'features']
weight: [1.]
kappa: [272.14294989]
['games', 'game', 'equilibrium', 'players', 'mechanism']
weight: [1.]
kappa: [213.30679187]
['channel', 'codes', 'rate', 'capacity', 'coding']
weight: [1.]
kappa: [195.851135]
['reasoning', 'knowledge', 'decision', 'belief', 'inference']
weight: [1.]
kappa: [267.13624818]
['computing', 'distributed', 'cloud', 'parallel', 'applications']
weight: [1.]
kappa: [158.77609893]
['language', 'word', 'text', 'words', 'translation']
weight: [1.]
kappa: [246.07450565]
['networks', 'wireless', 'routing', 'nodes', 'traffic']
weight: [1.]
kappa: [278.74059103]
['software', 'development', 'process', 'engineering', 'requirements']
weight: [1.]
kappa: [203.52568822]
['complexity', 'polynomial', 'np', 'prove', 'lower']
weight: [1.]
kappa: [123.24827152]
['security', 'attacks', 'attack', 'secure', 'key']
weight: [1.]
kappa: [241.61210932]
['classification', 'training', 'methods', 'machine', 'propose']
weight: [1.]
kappa: [134.43679109]
['proof', 'semantics', 'calculus', 'properties', 'logics']
weight: [1.]
kappa: [145.37618533]
['control', 'controller', 'stability', 'state', 'controllers']
weight: [1.]
kappa: [204.4226873]
['students', 'technology', 'research', 'social', 'education']
weight: [1.]
kappa: [231.66102528]
['log', 'approximation', 'graphs', 'given', 'size']
weight: [1.]
kappa: [117.37362631]
['programming', 'programs', 'language', 'program', 'languages']
weight: [1.]
kappa: [348.12575588]
['process', 'technology', 'software', 'use', 'high']
weight: [1.]
kappa: [120.28871666]
['query', 'database', 'queries', 'databases', 'mining']
weight: [1.]
kappa: [348.76515661]
Finished vMF distribution fitting.
Finished pseudo documents generation.

### Phase 2: pre-training with pseudo documents ###
Pretraining node cs
C:\Users\User\anaconda3\envs\NER\lib\site-packages\tensorflow\python\keras\optimizer_v2\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  warnings.warn(

Pretraining...
Epoch 1/30
1/1 [==============================] - 1s 1s/step - loss: 1.8715
Epoch 2/30
1/1 [==============================] - 0s 30ms/step - loss: 1.8690
Epoch 3/30
1/1 [==============================] - 0s 30ms/step - loss: 1.8655
Epoch 4/30
1/1 [==============================] - 0s 31ms/step - loss: 1.8605
Epoch 5/30
1/1 [==============================] - 0s 35ms/step - loss: 1.8551
Epoch 6/30
1/1 [==============================] - 0s 34ms/step - loss: 1.8494
Epoch 7/30
1/1 [==============================] - 0s 42ms/step - loss: 1.8429
Epoch 8/30
1/1 [==============================] - 0s 41ms/step - loss: 1.8361
Epoch 9/30
1/1 [==============================] - 0s 42ms/step - loss: 1.8285
Epoch 10/30
1/1 [==============================] - 0s 42ms/step - loss: 1.8199
Epoch 11/30
1/1 [==============================] - 0s 39ms/step - loss: 1.8109
Epoch 12/30
1/1 [==============================] - 0s 35ms/step - loss: 1.8015
Epoch 13/30
1/1 [==============================] - 0s 44ms/step - loss: 1.7911
Epoch 14/30
1/1 [==============================] - 0s 44ms/step - loss: 1.7800
Epoch 15/30
1/1 [==============================] - 0s 43ms/step - loss: 1.7684
Epoch 16/30
1/1 [==============================] - 0s 41ms/step - loss: 1.7558
Epoch 17/30
1/1 [==============================] - 0s 43ms/step - loss: 1.7423
Epoch 18/30
1/1 [==============================] - 0s 44ms/step - loss: 1.7282
Epoch 19/30
1/1 [==============================] - 0s 44ms/step - loss: 1.7132
Epoch 20/30
1/1 [==============================] - 0s 48ms/step - loss: 1.6969
Epoch 21/30
1/1 [==============================] - 0s 43ms/step - loss: 1.6794
Epoch 22/30
1/1 [==============================] - 0s 42ms/step - loss: 1.6610
Epoch 23/30
1/1 [==============================] - 0s 36ms/step - loss: 1.6414
Epoch 24/30
1/1 [==============================] - 0s 39ms/step - loss: 1.6202
Epoch 25/30
1/1 [==============================] - 0s 44ms/step - loss: 1.5974
Epoch 26/30
1/1 [==============================] - 0s 45ms/step - loss: 1.5731
Epoch 27/30
1/1 [==============================] - 0s 35ms/step - loss: 1.5474
Epoch 28/30
1/1 [==============================] - 0s 33ms/step - loss: 1.5196
Epoch 29/30
1/1 [==============================] - 0s 38ms/step - loss: 1.4898
Epoch 30/30
1/1 [==============================] - 0s 39ms/step - loss: 1.4582
Pretraining time: 2.76s

### Phase 3: self-training ###
Update interval: 30

Iter 0:
f1_macro = 0.12397, f1_micro = 0.22834 @ level 1
f1_macro = 0.04252, f1_micro = 0.05746 @ level 2
f1_macro = 0.04753, f1_micro = 0.08868 @ all classes

Iter 30:
f1_macro = 0.12394, f1_micro = 0.22823 @ level 1
f1_macro = 0.04384, f1_micro = 0.05831 @ level 2
f1_macro = 0.04877, f1_micro = 0.09007 @ all classes
Fraction of documents with label changes: 5.346 %
Final model saved to: ./results/arxiv/keywords/level_1/final.h5
Self-training time: 1064.74s
Classification results are written in ./arxiv\out.txt